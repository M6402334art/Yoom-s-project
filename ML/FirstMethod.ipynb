{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import libraly***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Read data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = pd.read_csv('../files/Good1.csv', nrows=30)\n",
    "g2 = pd.read_csv('../files/Good2.csv', nrows=30)\n",
    "g3 = pd.read_csv('../files/Good3.csv', nrows=30)\n",
    "g4 = pd.read_csv('../files/Good4.csv', nrows=30)\n",
    "g5 = pd.read_csv('../files/Good5.csv', nrows=30)\n",
    "g6 = pd.read_csv('../files/Good6.csv', nrows=30)\n",
    "g7 = pd.read_csv('../files/Good7.csv', nrows=30)\n",
    "g8 = pd.read_csv('../files/Good8.csv', nrows=30)\n",
    "g9 = pd.read_csv('../files/Good9.csv', nrows=30)\n",
    "g10 = pd.read_csv('../files/Good10.csv', nrows=30)\n",
    "g11 = pd.read_csv('../files/Good11.csv', nrows=30)\n",
    "g12 = pd.read_csv('../files/Good12.csv', nrows=30)\n",
    "g13 = pd.read_csv('../files/Good13.csv', nrows=30)\n",
    "g14 = pd.read_csv('../files/Good14.csv', nrows=30)\n",
    "g15 = pd.read_csv('../files/Good15.csv', nrows=30)\n",
    "g16 = pd.read_csv('../files/Good16.csv', nrows=30)\n",
    "g17 = pd.read_csv('../files/Good17.csv', nrows=30)\n",
    "g18 = pd.read_csv('../files/Good18.csv', nrows=30)\n",
    "g19 = pd.read_csv('../files/Good19.csv', nrows=30)\n",
    "g20 = pd.read_csv('../files/Good20.csv', nrows=30)\n",
    "g21 = pd.read_csv('../files/Good21.csv', nrows=30)\n",
    "g22 = pd.read_csv('../files/Good22.csv', nrows=30)\n",
    "g23 = pd.read_csv('../files/Good23.csv', nrows=30)\n",
    "g24 = pd.read_csv('../files/Good24.csv', nrows=30)\n",
    "g25 = pd.read_csv('../files/Good25.csv', nrows=30)\n",
    "g26 = pd.read_csv('../files/Good26.csv', nrows=30)\n",
    "g27 = pd.read_csv('../files/Good27.csv', nrows=30)\n",
    "g28 = pd.read_csv('../files/Good28.csv', nrows=30)\n",
    "g29 = pd.read_csv('../files/Good29.csv', nrows=30)\n",
    "g30 = pd.read_csv('../files/Good30.csv', nrows=30)\n",
    "b1 = pd.read_csv('../files/Bad1.csv', nrows=30)\n",
    "b2 = pd.read_csv('../files/Bad2.csv', nrows=30)\n",
    "b3 = pd.read_csv('../files/Bad3.csv', nrows=30)\n",
    "b4 = pd.read_csv('../files/Bad4.csv', nrows=30)\n",
    "b5 = pd.read_csv('../files/Bad5.csv', nrows=30)\n",
    "b6 = pd.read_csv('../files/Bad6.csv', nrows=30)\n",
    "b7 = pd.read_csv('../files/Bad7.csv', nrows=30)\n",
    "b8 = pd.read_csv('../files/Bad8.csv', nrows=30)\n",
    "b9 = pd.read_csv('../files/Bad9.csv', nrows=30)\n",
    "b10 = pd.read_csv('../files/Bad10.csv', nrows=30)\n",
    "b11 = pd.read_csv('../files/Bad11.csv', nrows=30)\n",
    "b12 = pd.read_csv('../files/Bad12.csv', nrows=30)\n",
    "b13 = pd.read_csv('../files/Bad13.csv', nrows=30)\n",
    "b14 = pd.read_csv('../files/Bad14.csv', nrows=30)\n",
    "b15 = pd.read_csv('../files/Bad15.csv', nrows=30)\n",
    "b16 = pd.read_csv('../files/Bad16.csv', nrows=30)\n",
    "b17 = pd.read_csv('../files/Bad17.csv', nrows=30)\n",
    "b18 = pd.read_csv('../files/Bad18.csv', nrows=30)\n",
    "b19 = pd.read_csv('../files/Bad19.csv', nrows=30)\n",
    "b20 = pd.read_csv('../files/Bad20.csv', nrows=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select mean because mean has a cluster cleary. (Compare between 'mean', 'mean', 'S.D.')\n",
    "# Create new dataframe\n",
    "\n",
    "HeatDmd = [\n",
    "    np.mean(g1['HeatDmd']), np.mean(g2['HeatDmd']), np.mean(g3['HeatDmd']), np.mean(g4['HeatDmd']),\n",
    "    np.mean(g5['HeatDmd']), np.mean(g6['HeatDmd']), np.mean(g7['HeatDmd']), np.mean(g8['HeatDmd']),\n",
    "    np.mean(g9['HeatDmd']), np.mean(g10['HeatDmd']), np.mean(g11['HeatDmd']), np.mean(g12['HeatDmd']),\n",
    "    np.mean(g13['HeatDmd']), np.mean(g14['HeatDmd']), np.mean(g15['HeatDmd']), np.mean(g16['HeatDmd']),\n",
    "    np.mean(g17['HeatDmd']), np.mean(g18['HeatDmd']), np.mean(g19['HeatDmd']), np.mean(g20['HeatDmd']),\n",
    "    np.mean(g21['HeatDmd']), np.mean(g22['HeatDmd']), np.mean(g23['HeatDmd']), np.mean(g24['HeatDmd']),\n",
    "    np.mean(g25['HeatDmd']), np.mean(g26['HeatDmd']),  np.mean(g27['HeatDmd']), np.mean(g28['HeatDmd']),\n",
    "    np.mean(g29['HeatDmd']),\n",
    "    np.mean(g30['HeatDmd']), np.mean(b1['HeatDmd']), np.mean(b2['HeatDmd']), np.mean(b3['HeatDmd']),\n",
    "    np.mean(b4['HeatDmd']), np.mean(b5['HeatDmd']), np.mean(b6['HeatDmd']), np.mean(b7['HeatDmd']),\n",
    "    np.mean(b8['HeatDmd']), np.mean(b9['HeatDmd']), np.mean(b10['HeatDmd']), np.mean(b11['HeatDmd']),\n",
    "    np.mean(b12['HeatDmd']), np.mean(b13['HeatDmd']), np.mean(b14['HeatDmd']), np.mean(b15['HeatDmd']),\n",
    "    np.mean(b16['HeatDmd']), np.mean(b17['HeatDmd']), np.mean(b18['HeatDmd']), np.mean(b19['HeatDmd']),\n",
    "    np.mean(b20['HeatDmd']),\n",
    "]\n",
    "\n",
    "TargetTemp_Tenths = [\n",
    "    np.mean(g1['TargetTemp_Tenths']), np.mean(g2['TargetTemp_Tenths']), np.mean(g3['TargetTemp_Tenths']), np.mean(g4['TargetTemp_Tenths']),\n",
    "    np.mean(g5['TargetTemp_Tenths']), np.mean(g6['TargetTemp_Tenths']), np.mean(g7['TargetTemp_Tenths']), np.mean(g8['TargetTemp_Tenths']),\n",
    "    np.mean(g9['TargetTemp_Tenths']), np.mean(g10['TargetTemp_Tenths']), np.mean(g11['TargetTemp_Tenths']), np.mean(g12['TargetTemp_Tenths']),\n",
    "    np.mean(g13['TargetTemp_Tenths']), np.mean(g14['TargetTemp_Tenths']), np.mean(g15['TargetTemp_Tenths']), np.mean(g16['TargetTemp_Tenths']),\n",
    "    np.mean(g17['TargetTemp_Tenths']), np.mean(g18['TargetTemp_Tenths']), np.mean(g19['TargetTemp_Tenths']), np.mean(g20['TargetTemp_Tenths']),\n",
    "    np.mean(g21['TargetTemp_Tenths']), np.mean(g22['TargetTemp_Tenths']), np.mean(g23['TargetTemp_Tenths']), np.mean(g24['TargetTemp_Tenths']),\n",
    "    np.mean(g25['TargetTemp_Tenths']), np.mean(g26['TargetTemp_Tenths']),  np.mean(g27['TargetTemp_Tenths']), np.mean(g28['TargetTemp_Tenths']),\n",
    "    np.mean(g29['TargetTemp_Tenths']),\n",
    "    np.mean(g30['TargetTemp_Tenths']), np.mean(b1['TargetTemp_Tenths']), np.mean(b2['TargetTemp_Tenths']), np.mean(b3['TargetTemp_Tenths']),\n",
    "    np.mean(b4['TargetTemp_Tenths']), np.mean(b5['TargetTemp_Tenths']), np.mean(b6['TargetTemp_Tenths']), np.mean(b7['TargetTemp_Tenths']),\n",
    "    np.mean(b8['TargetTemp_Tenths']), np.mean(b9['TargetTemp_Tenths']), np.mean(b10['TargetTemp_Tenths']), np.mean(b11['TargetTemp_Tenths']),\n",
    "    np.mean(b12['TargetTemp_Tenths']), np.mean(b13['TargetTemp_Tenths']), np.mean(b14['TargetTemp_Tenths']), np.mean(b15['TargetTemp_Tenths']),\n",
    "    np.mean(b16['TargetTemp_Tenths']), np.mean(b17['TargetTemp_Tenths']), np.mean(b18['TargetTemp_Tenths']), np.mean(b19['TargetTemp_Tenths']),\n",
    "    np.mean(b20['TargetTemp_Tenths']),\n",
    "]\n",
    "\n",
    "FlowTemp_Tenths = [\n",
    "    np.mean(g1['FlowTemp_Tenths']), np.mean(g2['FlowTemp_Tenths']), np.mean(g3['FlowTemp_Tenths']), np.mean(g4['FlowTemp_Tenths']),\n",
    "    np.mean(g5['FlowTemp_Tenths']), np.mean(g6['FlowTemp_Tenths']), np.mean(g7['FlowTemp_Tenths']), np.mean(g8['FlowTemp_Tenths']),\n",
    "    np.mean(g9['FlowTemp_Tenths']), np.mean(g10['FlowTemp_Tenths']), np.mean(g11['FlowTemp_Tenths']), np.mean(g12['FlowTemp_Tenths']),\n",
    "    np.mean(g13['FlowTemp_Tenths']), np.mean(g14['FlowTemp_Tenths']), np.mean(g15['FlowTemp_Tenths']), np.mean(g16['FlowTemp_Tenths']),\n",
    "    np.mean(g17['FlowTemp_Tenths']), np.mean(g18['FlowTemp_Tenths']), np.mean(g19['FlowTemp_Tenths']), np.mean(g20['FlowTemp_Tenths']),\n",
    "    np.mean(g21['FlowTemp_Tenths']), np.mean(g22['FlowTemp_Tenths']), np.mean(g23['FlowTemp_Tenths']), np.mean(g24['FlowTemp_Tenths']),\n",
    "    np.mean(g25['FlowTemp_Tenths']), np.mean(g26['FlowTemp_Tenths']),  np.mean(g27['FlowTemp_Tenths']), np.mean(g28['FlowTemp_Tenths']),\n",
    "    np.mean(g29['FlowTemp_Tenths']),\n",
    "    np.mean(g30['FlowTemp_Tenths']), np.mean(b1['FlowTemp_Tenths']), np.mean(b2['FlowTemp_Tenths']), np.mean(b3['FlowTemp_Tenths']),\n",
    "    np.mean(b4['FlowTemp_Tenths']), np.mean(b5['FlowTemp_Tenths']), np.mean(b6['FlowTemp_Tenths']), np.mean(b7['FlowTemp_Tenths']),\n",
    "    np.mean(b8['FlowTemp_Tenths']), np.mean(b9['FlowTemp_Tenths']), np.mean(b10['FlowTemp_Tenths']), np.mean(b11['FlowTemp_Tenths']),\n",
    "    np.mean(b12['FlowTemp_Tenths']), np.mean(b13['FlowTemp_Tenths']), np.mean(b14['FlowTemp_Tenths']), np.mean(b15['FlowTemp_Tenths']),\n",
    "    np.mean(b16['FlowTemp_Tenths']), np.mean(b17['FlowTemp_Tenths']), np.mean(b18['FlowTemp_Tenths']), np.mean(b19['FlowTemp_Tenths']),\n",
    "    np.mean(b20['FlowTemp_Tenths']),\n",
    "]\n",
    "\n",
    "CellTotalHeater_mA  = [\n",
    "    np.mean(g1['CellTotalHeater_mA']), np.mean(g2['CellTotalHeater_mA']), np.mean(g3['CellTotalHeater_mA']), np.mean(g4['CellTotalHeater_mA']),\n",
    "    np.mean(g5['CellTotalHeater_mA']), np.mean(g6['CellTotalHeater_mA']), np.mean(g7['CellTotalHeater_mA']), np.mean(g8['CellTotalHeater_mA']),\n",
    "    np.mean(g9['CellTotalHeater_mA']), np.mean(g10['CellTotalHeater_mA']), np.mean(g11['CellTotalHeater_mA']), np.mean(g12['CellTotalHeater_mA']),\n",
    "    np.mean(g13['CellTotalHeater_mA']), np.mean(g14['CellTotalHeater_mA']), np.mean(g15['CellTotalHeater_mA']), np.mean(g16['CellTotalHeater_mA']),\n",
    "    np.mean(g17['CellTotalHeater_mA']), np.mean(g18['CellTotalHeater_mA']), np.mean(g19['CellTotalHeater_mA']), np.mean(g20['CellTotalHeater_mA']),\n",
    "    np.mean(g21['CellTotalHeater_mA']), np.mean(g22['CellTotalHeater_mA']), np.mean(g23['CellTotalHeater_mA']), np.mean(g24['CellTotalHeater_mA']),\n",
    "    np.mean(g25['CellTotalHeater_mA']), np.mean(g26['CellTotalHeater_mA']),  np.mean(g27['CellTotalHeater_mA']), np.mean(g28['CellTotalHeater_mA']),\n",
    "    np.mean(g29['CellTotalHeater_mA']),\n",
    "    np.mean(g30['CellTotalHeater_mA']), np.mean(b1['CellTotalHeater_mA']), np.mean(b2['CellTotalHeater_mA']), np.mean(b3['CellTotalHeater_mA']),\n",
    "    np.mean(b4['CellTotalHeater_mA']), np.mean(b5['CellTotalHeater_mA']), np.mean(b6['CellTotalHeater_mA']), np.mean(b7['CellTotalHeater_mA']),\n",
    "    np.mean(b8['CellTotalHeater_mA']), np.mean(b9['CellTotalHeater_mA']), np.mean(b10['CellTotalHeater_mA']), np.mean(b11['CellTotalHeater_mA']),\n",
    "    np.mean(b12['CellTotalHeater_mA']), np.mean(b13['CellTotalHeater_mA']), np.mean(b14['CellTotalHeater_mA']), np.mean(b15['CellTotalHeater_mA']),\n",
    "    np.mean(b16['CellTotalHeater_mA']), np.mean(b17['CellTotalHeater_mA']), np.mean(b18['CellTotalHeater_mA']), np.mean(b19['CellTotalHeater_mA']),\n",
    "    np.mean(b20['CellTotalHeater_mA']),\n",
    "]\n",
    "\n",
    "HeatDmd  = [\n",
    "    np.mean(g1['HeatDmd']), np.mean(g2['HeatDmd']), np.mean(g3['HeatDmd']), np.mean(g4['HeatDmd']),\n",
    "    np.mean(g5['HeatDmd']), np.mean(g6['HeatDmd']), np.mean(g7['HeatDmd']), np.mean(g8['HeatDmd']),\n",
    "    np.mean(g9['HeatDmd']), np.mean(g10['HeatDmd']), np.mean(g11['HeatDmd']), np.mean(g12['HeatDmd']),\n",
    "    np.mean(g13['HeatDmd']), np.mean(g14['HeatDmd']), np.mean(g15['HeatDmd']), np.mean(g16['HeatDmd']),\n",
    "    np.mean(g17['HeatDmd']), np.mean(g18['HeatDmd']), np.mean(g19['HeatDmd']), np.mean(g20['HeatDmd']),\n",
    "    np.mean(g21['HeatDmd']), np.mean(g22['HeatDmd']), np.mean(g23['HeatDmd']), np.mean(g24['HeatDmd']),\n",
    "    np.mean(g25['HeatDmd']), np.mean(g26['HeatDmd']),  np.mean(g27['HeatDmd']), np.mean(g28['HeatDmd']),\n",
    "    np.mean(g29['HeatDmd']),\n",
    "    np.mean(g30['HeatDmd']), np.mean(b1['HeatDmd']), np.mean(b2['HeatDmd']), np.mean(b3['HeatDmd']),\n",
    "    np.mean(b4['HeatDmd']), np.mean(b5['HeatDmd']), np.mean(b6['HeatDmd']), np.mean(b7['HeatDmd']),\n",
    "    np.mean(b8['HeatDmd']), np.mean(b9['HeatDmd']), np.mean(b10['HeatDmd']), np.mean(b11['HeatDmd']),\n",
    "    np.mean(b12['HeatDmd']), np.mean(b13['HeatDmd']), np.mean(b14['HeatDmd']), np.mean(b15['HeatDmd']),\n",
    "    np.mean(b16['HeatDmd']), np.mean(b17['HeatDmd']), np.mean(b18['HeatDmd']), np.mean(b19['HeatDmd']),\n",
    "    np.mean(b20['HeatDmd']),\n",
    "]\n",
    "\n",
    "CoolDmd  = [\n",
    "    np.mean(g1['CoolDmd']), np.mean(g2['CoolDmd']), np.mean(g3['CoolDmd']), np.mean(g4['CoolDmd']),\n",
    "    np.mean(g5['CoolDmd']), np.mean(g6['CoolDmd']), np.mean(g7['CoolDmd']), np.mean(g8['CoolDmd']),\n",
    "    np.mean(g9['CoolDmd']), np.mean(g10['CoolDmd']), np.mean(g11['CoolDmd']), np.mean(g12['CoolDmd']),\n",
    "    np.mean(g13['CoolDmd']), np.mean(g14['CoolDmd']), np.mean(g15['CoolDmd']), np.mean(g16['CoolDmd']),\n",
    "    np.mean(g17['CoolDmd']), np.mean(g18['CoolDmd']), np.mean(g19['CoolDmd']), np.mean(g20['CoolDmd']),\n",
    "    np.mean(g21['CoolDmd']), np.mean(g22['CoolDmd']), np.mean(g23['CoolDmd']), np.mean(g24['CoolDmd']),\n",
    "    np.mean(g25['CoolDmd']), np.mean(g26['CoolDmd']),  np.mean(g27['CoolDmd']), np.mean(g28['CoolDmd']),\n",
    "    np.mean(g29['CoolDmd']),\n",
    "    np.mean(g30['CoolDmd']), np.mean(b1['CoolDmd']), np.mean(b2['CoolDmd']), np.mean(b3['CoolDmd']),\n",
    "    np.mean(b4['CoolDmd']), np.mean(b5['CoolDmd']), np.mean(b6['CoolDmd']), np.mean(b7['CoolDmd']),\n",
    "    np.mean(b8['CoolDmd']), np.mean(b9['CoolDmd']), np.mean(b10['CoolDmd']), np.mean(b11['CoolDmd']),\n",
    "    np.mean(b12['CoolDmd']), np.mean(b13['CoolDmd']), np.mean(b14['CoolDmd']), np.mean(b15['CoolDmd']),\n",
    "    np.mean(b16['CoolDmd']), np.mean(b17['CoolDmd']), np.mean(b18['CoolDmd']), np.mean(b19['CoolDmd']),\n",
    "    np.mean(b20['CoolDmd']),\n",
    "]\n",
    "\n",
    "FTemp  = [\n",
    "    np.mean(g1['FTemp']), np.mean(g2['FTemp']), np.mean(g3['FTemp']), np.mean(g4['FTemp']),\n",
    "    np.mean(g5['FTemp']), np.mean(g6['FTemp']), np.mean(g7['FTemp']), np.mean(g8['FTemp']),\n",
    "    np.mean(g9['FTemp']), np.mean(g10['FTemp']), np.mean(g11['FTemp']), np.mean(g12['FTemp']),\n",
    "    np.mean(g13['FTemp']), np.mean(g14['FTemp']), np.mean(g15['FTemp']), np.mean(g16['FTemp']),\n",
    "    np.mean(g17['FTemp']), np.mean(g18['FTemp']), np.mean(g19['FTemp']), np.mean(g20['FTemp']),\n",
    "    np.mean(g21['FTemp']), np.mean(g22['FTemp']), np.mean(g23['FTemp']), np.mean(g24['FTemp']),\n",
    "    np.mean(g25['FTemp']), np.mean(g26['FTemp']),  np.mean(g27['FTemp']), np.mean(g28['FTemp']),\n",
    "    np.mean(g29['FTemp']),\n",
    "    np.mean(g30['FTemp']), np.mean(b1['FTemp']), np.mean(b2['FTemp']), np.mean(b3['FTemp']),\n",
    "    np.mean(b4['FTemp']), np.mean(b5['FTemp']), np.mean(b6['FTemp']), np.mean(b7['FTemp']),\n",
    "    np.mean(b8['FTemp']), np.mean(b9['FTemp']), np.mean(b10['FTemp']), np.mean(b11['FTemp']),\n",
    "    np.mean(b12['FTemp']), np.mean(b13['FTemp']), np.mean(b14['FTemp']), np.mean(b15['FTemp']),\n",
    "    np.mean(b16['FTemp']), np.mean(b17['FTemp']), np.mean(b18['FTemp']), np.mean(b19['FTemp']),\n",
    "    np.mean(b20['FTemp']),\n",
    "]\n",
    "\n",
    "# 0 is Bad, 1 is Good\n",
    "result = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "df1 = {'HeatDmd': HeatDmd, 'TargetTemp_Tenths': TargetTemp_Tenths, 'FlowTemp_Tenths': FlowTemp_Tenths, \n",
    "       'CoolDmd': CoolDmd,  'FTemp': FTemp, 'Output': result}\n",
    "df = pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate inputs and output\n",
    "X = df.drop(columns='Output') #Inputs are HeatDmd, CoolDmd, and Temp\n",
    "Y = df['Output'] # Output is Good or Bad (Binary classification)\n",
    "\n",
    "# Data Normalization\n",
    "sc = StandardScaler() # select standard (z-score)\n",
    "sc.fit(X)\n",
    "X_sc = sc.transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_sc, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional ML #\n",
    "\n",
    "# Logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# Decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "# SVM\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(x_train, y_train)\n",
    "\n",
    "# Naive bay\n",
    "naive = GaussianNB()\n",
    "naive.fit(x_train, y_train)\n",
    "\n",
    "# ANN\n",
    "ann = MLPClassifier(\n",
    "     hidden_layer_sizes=(200,),\n",
    "        max_iter=500,              # Set your desired maximum number of iterations\n",
    "        random_state=42,\n",
    "        alpha=0.001,              \n",
    "        solver='adam',             \n",
    "        learning_rate_init=0.001,   \n",
    "        tol=1e-4,                   \n",
    "        early_stopping=False, \n",
    ")\n",
    "ann.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning #\n",
    "\n",
    "# ANN\n",
    "hidden_layer_sizes = [50, 100, 150, 200]\n",
    "loss_values = {size: [] for size in hidden_layer_sizes}\n",
    "accuracy_values = {size: [] for size in hidden_layer_sizes}\n",
    "roc_auc_values = {size: [] for size in hidden_layer_sizes}\n",
    "\n",
    "confusion_matrices = {size: None for size in hidden_layer_sizes}\n",
    "# Iterate over hidden layer sizes\n",
    "for size in hidden_layer_sizes:\n",
    "    print(f\"\\nTraining with hidden layer size: {size}\")\n",
    "    \n",
    "    # Initialize model with the current hidden layer size\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(size,),\n",
    "        max_iter=500,              # Set your desired maximum number of iterations\n",
    "        random_state=42,\n",
    "        alpha=0.001,               # L2 regularization strength\n",
    "        solver='adam',             # Adam optimizer\n",
    "        learning_rate_init=0.001,   # Learning rate\n",
    "        tol=1e-4,                   # Tolerance for convergence\n",
    "        early_stopping=False,       # No early stopping\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Save the confusion matrix in the dictionary\n",
    "    confusion_matrices[size] = confusion_matrix_result\n",
    "    \n",
    "    # Display confusion matrix for the current hidden layer size\n",
    "    print(f\"\\nConfusion Matrix for Hidden Layer Size {size}:\\n\")\n",
    "    print(confusion_matrix_result)\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    # Fit the model and store loss and accuracy at each iteration\n",
    "    classes_initialized = False\n",
    "    for iteration in range(1, model.max_iter + 1):\n",
    "        if not classes_initialized:\n",
    "            # On the first iteration, pass the classes to partial_fit\n",
    "            model.partial_fit(x_train, y_train, classes=df['Output'].unique())\n",
    "            classes_initialized = True\n",
    "        else:\n",
    "            # For subsequent iterations, use partial_fit without classes\n",
    "            model.partial_fit(x_train, y_train)\n",
    "\n",
    "        # Calculate loss on the training set\n",
    "        loss_values[size].append(model.loss_)\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_values[size].append(accuracy)\n",
    "\n",
    "        # Calculate ROC curve and AUC for binary classification\n",
    "        if len(set(y_train)) == 2:\n",
    "            y_bin = label_binarize(y_test, classes=list(set(y_train)))\n",
    "            fpr, tpr, _ = roc_curve(y_bin.ravel(), model.predict_proba(x_test)[:, 1])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_auc_values[size].append(roc_auc)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Testing\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(x_test)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    # Print training and test times\n",
    "    print(f\"Training Time: {training_time * 1e6:.2f} microseconds\")\n",
    "    print(f\"Test Time: {test_time * 1e6:.2f} microseconds\")\n",
    "    \n",
    "    # Print number of train and test data\n",
    "    print(f\"Number of Train Data: {len(x_train)}\")\n",
    "    print(f\"Number of Test Data: {len(x_test)}\")\n",
    "    \n",
    "    # Print F1 score and accuracy\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "markers = ['o', '^', '*', '+']\n",
    "for i, (size, values) in enumerate(loss_values.items()):\n",
    "    plt.plot(range(50, model.max_iter + 1, 50), values[49::50], marker=markers[i], label=f'Hidden Layer Size {size}')\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot Accuracy Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "for i, (size, values) in enumerate(accuracy_values.items()):\n",
    "    plt.plot(range(50, model.max_iter + 1, 50), values[49::50], marker=markers[i], label=f'Hidden Layer Size {size}')\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.subplot(1, 3, 3)\n",
    "if len(set(y_train)) == 2:\n",
    "    for i, (size, values) in enumerate(roc_auc_values.items()):\n",
    "        plt.plot(range(50, model.max_iter + 1, 50), values[49::50], marker=markers[i], label=f'Hidden Layer Size {size}')\n",
    "\n",
    "    plt.title('ROC AUC')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "lr_predict = lr.predict(x_test)\n",
    "lr_acc = accuracy_score(y_true=y_test, y_pred=lr_predict)\n",
    "\n",
    "# KNN\n",
    "knn_predict = knn.predict(x_test)\n",
    "knn_acc = accuracy_score(y_true=y_test, y_pred=knn_predict)\n",
    "\n",
    "# Decision tree\n",
    "tree_predict = tree.predict(x_test)\n",
    "tree_acc = accuracy_score(y_true=y_test, y_pred=tree_predict)\n",
    "\n",
    "# SVM\n",
    "svm_predict = svm.predict(x_test)\n",
    "svm_acc = accuracy_score(y_true=y_test, y_pred=svm_predict)\n",
    "\n",
    "# Naive bay\n",
    "naive_predict = naive.predict(x_test)\n",
    "naive_acc = accuracy_score(y_true=y_test, y_pred=naive_predict)\n",
    "\n",
    "#ANN\n",
    "ann_predict = ann.predict(x_test)\n",
    "ann_acc = accuracy_score(y_true=y_test, y_pred=ann_predict)\n",
    "\n",
    "print ('Logistic = %.2f, KNN = %.2f, Tree = %.2f, SVM = %.2f , Naive = %.2f, ANN = %.2f' %(lr_acc*100, knn_acc*100, tree_acc*100, svm_acc*100, naive_acc*100, ann_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(15,10))\n",
    "\n",
    "# Logistic regression\n",
    "lr_cm = confusion_matrix(y_test, lr_predict, labels=lr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=lr_cm,\n",
    "                              display_labels=lr.classes_)\n",
    "disp.plot(ax=axs[0,0])\n",
    "axs[0,0].set_title(f'Logistic = {lr_acc*100}')\n",
    "\n",
    "# KNN\n",
    "knn_cm = confusion_matrix(y_test, knn_predict, labels=knn.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=knn_cm,\n",
    "                              display_labels=knn.classes_)\n",
    "disp.plot(ax=axs[0,1])\n",
    "axs[0,1].set_title(f'KNN = {knn_acc*100}')\n",
    "\n",
    "# Decision tree\n",
    "tree_cm = confusion_matrix(y_test, tree_predict, labels=tree.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=tree_cm,\n",
    "                              display_labels=tree.classes_)\n",
    "disp.plot(ax=axs[0,2])\n",
    "axs[0,2].set_title(f'Tree = {tree_acc*100}')\n",
    "\n",
    "# SVM\n",
    "svm_cm = confusion_matrix(y_test, svm_predict, labels=svm.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=svm_cm,\n",
    "                              display_labels=svm.classes_)\n",
    "disp.plot(ax=axs[1,0])\n",
    "axs[1,0].set_title(f'SVM = {svm_acc*100}')\n",
    "\n",
    "# Naive bay\n",
    "naive_cm = confusion_matrix(y_test, naive_predict, labels=naive.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=naive_cm,\n",
    "                              display_labels=naive.classes_)\n",
    "disp.plot(ax=axs[1,1])\n",
    "axs[1,1].set_title(f'Naive = {naive_acc*100}')\n",
    "\n",
    "# ANN\n",
    "ann_cm = confusion_matrix(y_test, ann_predict, labels=ann.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=ann_cm,\n",
    "                              display_labels=ann.classes_)\n",
    "disp.plot(ax=axs[1,2])\n",
    "axs[1,2].set_title(f'ANN = {ann_acc*100}')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFile = pd.read_csv('../test/B1.csv', nrows=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeatDmd = [np.mean(testFile['HeatDmd'])]\n",
    "TargetTemp_Tenths = [np.mean(testFile['TargetTemp_Tenths'])]\n",
    "FlowTemp_Tenths = [np.mean(testFile['FlowTemp_Tenths'])]\n",
    "CoolDmd = [np.mean(testFile['CoolDmd'])]\n",
    "FTemp = [np.mean(testFile['FTemp'])]\n",
    "\n",
    "\n",
    "dfDic = {'HeatDmd': HeatDmd, 'TargetTemp_Tenths': TargetTemp_Tenths, 'FlowTemp_Tenths': FlowTemp_Tenths, \n",
    "       'CoolDmd': CoolDmd,  'FTemp': FTemp}\n",
    "dfTest = pd.DataFrame(dfDic)\n",
    "\n",
    "dfTestSC = sc.transform(dfTest)\n",
    "predictionTest = ann.predict(dfTestSC)\n",
    "confidence_levels = ann.predict_proba(dfTestSC)\n",
    "print(f'prediction: {predictionTest}, confidence: {confidence_levels.max(axis=1)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Export models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export scaler model\n",
    "joblib.dump(sc, '../models/scaler.pkl')\n",
    "\n",
    "#export machine learning model\n",
    "joblib.dump(ann, '../models/best.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
